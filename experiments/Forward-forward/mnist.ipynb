{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward-forward -- Backpropgation Alternative\n",
    "\n",
    "It is December of 2022, The Godfather of AI, Geoffrey Hinton, has dumped some more wisdom about wisdom into the Universe by way of the `Forward-forward` algorithm\n",
    "\n",
    "This algorithm for training Neural Networks performs (based on preliminary results) slighty worse than backprop but works via some very different properties that perhaps make it very attractive for specific applications\n",
    "\n",
    "\n",
    "## Links\n",
    "- Paper- https://www.cs.toronto.edu/~hinton/FFA13.pdf\n",
    "- Reference Repo- https://github.com/mohammadpz/pytorch_forward_forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-forward Implementation in Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Vars\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random_seed = 42\n",
    "\n",
    "# Util to Load in MNIST Data --> used in Hinton's example\n",
    "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
    "\n",
    "    # Standard Transformations to the MNIST Dataset\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    # Instantiate Loaders (can call these for randomized batches at train/inference time)\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Add the Label information to X Data (mentioned in Hinton's paper, see Section 3.3)\n",
    "def overlay_y_on_x(x, y):\n",
    "    \"\"\"\n",
    "    x is a single training instance (MNIST Vec of len 784) and y is the scalar value representing the label \n",
    "    \"\"\"\n",
    "    x_ = x.clone()\n",
    "    x_[:, :10] *= 0.0 #two classes for Moons -- would be 10 for MNIST\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_\n",
    "\n",
    "\n",
    "# Get Loaders for Datasets\n",
    "train_loader, test_loader = MNIST_loaders() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Class that Instantiates our Custom Layers Class & Implements the Train/Predict Funcs\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        # Append Layers w Correct Dimensions for Weight Matrices -- dims is a list of ints (i.e; list[int])\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
    "\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            h = overlay_y_on_x(x, label) #put current label in iteration on training instance\n",
    "            goodness = []\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "                goodness += [h.pow(2).mean(1)] #sum of squared \"goodness\" -- take max for pred\n",
    "            # Compute Goodness for Current Label in Range Iters\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "\n",
    "        # Get Goodness Over all Labels \n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(1)\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print('training layer', i, '...')\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
    "\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=0.03)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 1000\n",
    "        # self.num_epochs = 10000 #10x increase in epochs, what do?\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        return self.relu(\n",
    "            torch.mm(x_direction, self.weight.T) +\n",
    "            self.bias.unsqueeze(0))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        for i in tqdm(range(self.num_epochs)):\n",
    "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
    "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
    "            # The following loss pushes pos (neg) samples to\n",
    "            # values larger (smaller) than the self.threshold.\n",
    "            loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                -g_pos + self.threshold,\n",
    "                g_neg - self.threshold]))).mean()\n",
    "            self.opt.zero_grad()\n",
    "            # this backward just compute the derivative and hence\n",
    "            # is not considered backpropagation.\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 22.03it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 7.78 GiB total capacity; 5.41 GiB already allocated; 172.12 MiB free; 5.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Go Forward-forward\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m net\u001b[39m.\u001b[39mtrain(x_pos, x_neg)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain error:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m net\u001b[39m.\u001b[39;49mpredict(x)\u001b[39m.\u001b[39meq(y)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m x_te, y_te \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(test_loader))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m x_te, y_te \u001b[39m=\u001b[39m x_te\u001b[39m.\u001b[39mto(device), y_te\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb Cell 4\u001b[0m in \u001b[0;36mNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m goodness \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     h \u001b[39m=\u001b[39m layer(h)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     goodness \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [h\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m)] \u001b[39m#sum of squared \"goodness\" -- take max for pred\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Compute Goodness for Current Label in Range Iters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb Cell 4\u001b[0m in \u001b[0;36mLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     x_direction \u001b[39m=\u001b[39m x \u001b[39m/\u001b[39m (x\u001b[39m.\u001b[39mnorm(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39m1e-4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m         torch\u001b[39m.\u001b[39;49mmm(x_direction, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mT) \u001b[39m+\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.26/home/ckg/Desktop/dl-pt/experiments/Forward-forward/mnist.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 7.78 GiB total capacity; 5.41 GiB already allocated; 172.12 MiB free; 5.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Main Training Loop -- Moons\n",
    "\n",
    "# torch.manual_seed(1234)\n",
    "torch.manual_seed(42)\n",
    "train_loader, test_loader = MNIST_loaders() #loaders load in the entirety of the MNIST Set\n",
    "\n",
    "# Instantiate Model + Data\n",
    "net = Net([784, 500, 500])\n",
    "net = Net([784, 1000, 1000])\n",
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "x_pos = overlay_y_on_x(x, y) #add actual labels to training instances\n",
    "\n",
    "\n",
    "# Create Random Label for x_negative\n",
    "rnd = torch.randperm(x.size(0))\n",
    "x_neg = overlay_y_on_x(x, y[rnd])\n",
    "\n",
    "# Go Forward-forward\n",
    "net.train(x_pos, x_neg)\n",
    "\n",
    "print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(test_loader))\n",
    "x_te, y_te = x_te.to(device), y_te.to(device)\n",
    "\n",
    "print('test error:', 1.0 - net.predict(x_te).eq(y_te).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26343712b899080af65fbf5163bf31a389a3681fe88f5cd99452d52639027dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
