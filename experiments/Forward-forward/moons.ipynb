{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out the Forward-forward algo on Make_Moons' Toy Data\n",
    "\n",
    "It doesn't seem to work all that well -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-forward Implementation in Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "\n",
    "# Vars\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "# Util to Load in MNIST Data --> used in Hinton's example\n",
    "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
    "\n",
    "    # Standard Transformations to the MNIST Dataset\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    # Instantiate Loaders (can call these for randomized batches at train/inference time)\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Toy Dataset\n",
    "def MOON_dataset(train_size=50000, test_size=10000):\n",
    "    n_total = train_size + test_size\n",
    "    x, y = make_moons(n_total, noise=0.0, random_state=random_seed)\n",
    "    # x, y = make_blobs(n_samples=n_total, random_state=42) #not the traditional moon but reasonable to learn\n",
    "\n",
    "    trainset = (torch.tensor(x[:train_size]), torch.tensor(y[:train_size]))\n",
    "    testset =  (torch.tensor(x[-test_size:]), torch.tensor(y[-test_size:]))\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "# Get Loaders for Datasets\n",
    "# train_loader, test_loader = MNIST_loaders() \n",
    "train_loader, test_loader = MOON_dataset() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Label information to X Data (mentioned in Hinton's paper, see Section 3.3)\n",
    "def overlay_y_on_x(x, y):\n",
    "    \"\"\"\n",
    "    x is a single training instance (MNIST Vec of len 784) and y is the scalar value representing the label \n",
    "    \"\"\"\n",
    "    x_ = x.clone()\n",
    "    x_[:, :10] *= 0.0 #two classes for Moons -- would be 10 for MNIST\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_\n",
    "\n",
    "def insert_moon_label(x, y):\n",
    "    \"method to insert the label for moons dataset\"\n",
    "    x_train_insert = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        new_data = x[i].tolist()\n",
    "        new_data.append(y[i].item())\n",
    "        x_train_insert.append(new_data)\n",
    "    return torch.tensor(x_train_insert)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF Code from Repo\n",
    "# Network Class that Instantiates our Custom Layers Class & Implements the Train/Predict Funcs\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        # Append Layers w Correct Dimensions for Weight Matrices -- dims is a list of ints (i.e; list[int])\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
    "\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(2):\n",
    "            h = overlay_y_on_x(x, label) #put current label in iteration on training instance\n",
    "            goodness = []\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "                goodness += [h.pow(2).mean(1)] #sum of squared \"goodness\" -- take max for pred\n",
    "            # Compute Goodness for Current Label in Range Iters\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "\n",
    "        # Get Goodness Over all Labels \n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(1)\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print('training layer', i, '...')\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
    "\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=0.05)\n",
    "        self.threshold = 2.0\n",
    "        # self.num_epochs = 1000\n",
    "        self.num_epochs = 1000 #10x increase in epochs, what do?\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        return self.relu(\n",
    "            torch.mm(x_direction, self.weight.T) +\n",
    "            self.bias.unsqueeze(0))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        for i in tqdm(range(self.num_epochs)):\n",
    "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
    "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
    "            # The following loss pushes pos (neg) samples to\n",
    "            # values larger (smaller) than the self.threshold.\n",
    "            loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                -g_pos + self.threshold,\n",
    "                g_neg - self.threshold]))).mean()\n",
    "            self.opt.zero_grad()\n",
    "            # this backward just compute the derivative and hence\n",
    "            # is not considered backpropagation.\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 97.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 55.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.4980599880218506\n",
      "test error: 0.5097000002861023\n"
     ]
    }
   ],
   "source": [
    "# Main Training Loop -- Moons\n",
    "\n",
    "# train_loader, test_loader = MNIST_loaders() #loaders load in the entirety of the MNIST Set\n",
    "\n",
    "# Instantiate Model + Data\n",
    "net = Net([3, 500, 500])\n",
    "# x, y = next(iter(train_loader))\n",
    "x, y = train_loader[0], train_loader[1]\n",
    "x, y = x.cuda(), y.cuda()\n",
    "\n",
    "# x_pos = overlay_y_on_x(x, y) #add actual labels to training instances\n",
    "x_pos = insert_moon_label(x, y) #add actual labels to training instances\n",
    "x_pos = x_pos.to(device)\n",
    "\n",
    "# Create Random Label for x_negative\n",
    "rnd = torch.randperm(x.size(0))\n",
    "x_neg = insert_moon_label(x, y[rnd])\n",
    "x_neg = x_neg.to(device)\n",
    "\n",
    "\n",
    "# Go Forward-forward\n",
    "net.train(x_pos, x_neg)\n",
    "\n",
    "# y_eval = torch.zeros(x.shape[0])\n",
    "y_eval = torch.rand(x.shape[0]) #does including a bit of stochasticity make it better?\n",
    "x_eval = insert_moon_label(x, y_eval)\n",
    "x_eval = x_eval.to(device)\n",
    "\n",
    "# print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item()) #switched to below, need include extra dim\n",
    "print('train error:', 1.0 - net.predict(x_eval).eq(y).float().mean().item())\n",
    "\n",
    "# x_te, y_te = next(iter(test_loader))\n",
    "x_test, y_test = test_loader[0], test_loader[1]\n",
    "x_test, y_test = x_test.cuda(), y_test.cuda()\n",
    "x_test = insert_moon_label(x_test, y_test) #add actual labels to training instances\n",
    "x_test = x_test.to(device)\n",
    "\n",
    "print('test error:', 1.0 - net.predict(x_test).eq(y_test).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26343712b899080af65fbf5163bf31a389a3681fe88f5cd99452d52639027dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
