{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchBook\n",
    "\n",
    "A Spellbook for Torch (basically local lookup of my own stuff, stackoverflow posts & docs for torchy things)\n",
    "\n",
    "- Each cell should roughly be a standalone runable (copy and paste to the ether and will work)\n",
    "\n",
    "\n",
    "がんばって\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors \n",
    "\n",
    "\n",
    "+ Official Docs- https://pytorch.org/docs/stable/tensors.html \n",
    "+ Nice Tutorial- https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "+ basically numpy arrays (even share memory address if on CPU, could be problematic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tensor from Existing ndarray-like object (np.ndarrays or list of lists, etc.)\n",
    "\n",
    "data = [[1, 2], [3, 4], [5, 6]] #list of lists work\n",
    "x_data = torch.tensor(data) \n",
    "\n",
    "# or if in Numpy\n",
    "np_array = np.array(data) #can also read from a numpy array (easy to connect to a numerically encoded PandasFrame)\n",
    "x_data = torch.tensor(np_array) #same call as list-of-lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Tensors w/o Existing Data\n",
    "# More Methods- https://www.geeksforgeeks.org/creating-a-tensor-in-pytorch/\n",
    "\n",
    "torch.randint(low=0, high=10, size=(3, 1, 5)) #create a tensor of shape [3x1x5] w random values between 0 & 10\n",
    "torch.eye(n=5, m=5) #generate a identity matrix of shape nxm (diagonals=1, other_values=0)\n",
    "torch.zeros(3,2) #returns tensor of zeroes in specified shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Tensor Based on an Existing One (transformed in similar shape) -- \"_like\" is moniker for creating tes\n",
    "\n",
    "misc_data = torch.randint(low=-3, high=3, size=(7, 2))\n",
    "\n",
    "x_ones = torch.ones_like(misc_data) #returns a tensor of same shape as x_data with all values (x_ij) = 1\n",
    "x_rand = torch.rand_like(misc_data, dtype=torch.float) #returns same shape tensor with random values, can change dtype as well\n",
    "\n",
    "print(x_ones, x_rand, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = None\n"
     ]
    }
   ],
   "source": [
    "# Tensor Attributes\n",
    "\n",
    "t = torch.rand(4, 5)\n",
    "\n",
    "t.shape #the dims of the Tensor (specified in above function)\n",
    "t.device #where tensor is at the moment, either: [\"cpu\", \"cuda\"+device_index] (index provided since multi-gpu is possibility)\n",
    "t.dtype #data-type for tensor, supported types are listed in docs\n",
    "t.requires_grad #determines if this tensor requires gradient calculation (takes derivative in forward pass if model in train mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Functions -- callable methods available within Tensor\n",
    "\n",
    "t = torch.rand(2, 10)\n",
    "\n",
    "# Send a Tensor to Device (CPU or GPU)\n",
    "compute_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "t.to(compute_device) #send tensor/object to GPU or CPU, use above to specify (can also include index)\n",
    "\n",
    "\n",
    "# Transformations & Math Operations\n",
    "t.T #linalg-like transpose for a vector/matrix, can get finicky in higher dim tensors\n",
    "t.matmul(t.T) #make sure dims are correct for matmul (vector needs multiply w transposed vector, i.e. Nx1 * 1XN)\n",
    "\n",
    "t * t #elementwise product, need correct dims!\n",
    "t.mul(t) #alt syntax for elementwise product\n",
    "\n",
    "t.abs() #returns the absolute value of all items in tensor (converts negs into pos), same dim for returned tensor\n",
    "\n",
    "sum_of_t = t.sum() #computes sum of all values in t (all dims condensed into a \"scalar\")\n",
    "sum_of_t.item() #returns the value (python-like float, int, etc.) if this Tensor is a scalar (shape = \"torch.Size([])\")\n",
    "\n",
    "torch.cat([t, t], dim=0) #concatenates N tensors (specified in list) along a specified dim (need same dimensions for sub-dims, but not necessarily same 'len' for cat dim)\n",
    "\n",
    "# These are `in-place`operations, that is the _ in each of the functions does the operation without needing assignment, can omit the underscore for regular python behavior\n",
    "t.add_(42) #adds the specified value to all elements in the tensor\n",
    "t.subtract_(2) #same for subtraction\n",
    "t.mul_(2) #same for scaling multiplication\n",
    "t.div_(2) #same for scaling division, need a float dtype!\n",
    "\n",
    "\n",
    "# Numpy Compatiblity -- if Tensor on CPU, any changes made to np array will be made to Tensor (same object in low-level memory)\n",
    "np_t = t.numpy() #create new np array from tensor (this is a method of tensor, below is a torch method!)\n",
    "t = torch.from_numpy(np_t) #alt way of creating tensor from an np.ndarray\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing Tensors\n",
    "\n",
    "t = torch.rand(10, 2, 2)\n",
    "\n",
    "t[2] #access one of the ^ 10 Matricies of shape 2x2 (total tensor shape is 10x2x2 but we index each dim, this case is first dim)\n",
    "t[0][1][1] #access an single item in dim 3, given prev dims we want to index\n",
    "\n",
    "t[1:4] #slice based on an arbitrary dimension (0th indexed like numpy, so for accessing the \"10th\" element we use: \"t[9]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & DataLoaders\n",
    "\n",
    "Standardized way for interacting with external databases (in RL or similar scenario we could just use tensors for holding data, but images or audio need a wrapper for this layer)\n",
    "\n",
    "- Data Docs- https://pytorch.org/docs/stable/data.html\n",
    "- Related Tutorial– https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "- Custom Datasets are not too horrible to right\n",
    "- Torch supports two types of \"DataSet\" styles, that is they have\n",
    "  - [Iterable-Style](https://pytorch.org/docs/stable/data.html#iterable-style-datasets)\n",
    "  - [Map-Style](https://pytorch.org/docs/stable/data.html#map-style-datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for a File Directory - Need Implement (AT MIN) the following in a Custom SubClass\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# BoilerPlate Code -- Need finagle for any unique usage\n",
    "class CustomImageDataset(Dataset):\n",
    "    # Need an `__init__` method w these params written (dir for data, any custom transforms, mapping of labels, etc.)\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file) #very important! need filepaths for instances and labels in this CSV\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    # Total Number of Instances (need count of this for other ops)\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    # Where to Fetch a instance and what transforms to apply\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataloader from Tensor Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train) #items here are \"torch.tensor\"'s of a PandasFrame's data\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True) #need specify `batch_size` param\n",
    "\n",
    "# View a Sampled Set of Data\n",
    "for i_batch, samples in enumerate(train_loader):\n",
    "    print(\"Batch:\", i_batch)\n",
    "    print(\"Data:\", samples[0].shape)\n",
    "    print(\"Labels:\", samples[1])\n",
    "\n",
    "# Quick test -- get samples and labels (quicker method of iterating through)\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing a DataSet Object for Specific Instances\n",
    "\n",
    "train_set[500][1] #this is parsed as follows: test_data[instance][0=data, 1=label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Shape for Items in a single iter of a DataLoader\n",
    "[i.shape for i in next(iter(train_loader))] #returns the shapes of the input data and correct output/label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "\n",
    "Torch is the superior framework\n",
    "\n",
    "- Models Docs- https://pytorch.org/docs/stable/nn.html\n",
    "- If doing training interminently (\"warm start\" training as the cool kids call it), reference- https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Model Architecture Code\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Define NN Architecture w Regularization (not making use of sequential API for layer calls -- done manually in `forward`)\n",
    "                                           # Sequential API- https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=False, dropout_rate=0.5, batchnorm=False, activation=None):\n",
    "        super(FFN, self).__init__()\n",
    "        \n",
    "        # Network Architecture (Layers + HardCoded Activations) -- 1 Hidden Layer Here \n",
    "        self.fc = nn.Linear(input_size, hidden_size) #input-to-hidden Weight Matrix\n",
    "        self.h1 = nn.Linear(hidden_size, output_size) #hidden-to-output Weight Matrix\n",
    "        \n",
    "        # Activation Functions -- Full List @ https://pytorch.org/docs/stable/nn.html\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax() #useful for multi-class probabilties\n",
    "\n",
    "        # Regularization\n",
    "        self.batch_norm = nn.BatchNorm1d(output_size) if batchnorm else None\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout else None\n",
    "        self.activation = activation\n",
    "        self.flatten = nn.Flatten() #nice way of making sure input to model is always a vector!\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute Forward Pass\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.h1(x)\n",
    "        # x = self.sigmoid(x) #apply on output -- learning may fail if applied (vanishing gradient, predicts only one class for some architectures)\n",
    "                              #learning (in some cases) may benefit from no output activation, larger gradient updates through backprop\n",
    "                              \n",
    "        # Apply Regularization (if specified)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x) #calls activation function at end of forward pass\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x #output for given input\n",
    "\n",
    "\n",
    "# Calling a Forward Pass w Model\n",
    "model(x_train) #this is the syntax for a forward pass, implicitly calls `forward` function, calling `forward` directly is discouraged as this method also calls background ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Param Summary (Keras-like -- can install sep module w `pip install torchsummary`)\n",
    "import torchinfo \n",
    "\n",
    "torchinfo.summary(model) #can be issues w this if GPU, check on all available devices if being finicky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters - The Usual Suspects\n",
    "# (these govern how a model selects parameters)\n",
    "\n",
    "learning_rate = 7e-3 #7e-3=0.007 hehe, setting this varies, generally depends on optimizer of choice\n",
    "batch_size = 8 #batch_size of 1 means we train (update model params) on each instance, if larger we accumulate gradients and update params per batch (larger~=faster, whereas smaller~=more learning)\n",
    "n_epochs = 10 #number of epochs to train for (total number of training iterations over entire dataset)\n",
    "\n",
    "# Regularization HyperParams\n",
    "dropout_rate = 0.8 #proportion of neurons to drop out (in this formulation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions - https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "# Instantiate Loss Function\n",
    "loss_fn = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "# Backpropagate Loss\n",
    "loss.backward() #after the optimizer has accumulated gradients wrt model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing Algorithms - https://pytorch.org/docs/stable/optim.html#algorithmsl\n",
    "\n",
    "# Instantiate an `optimizer`\n",
    "optimizer = torch.optim.adam(model.parameters(), lr=learning_rate) #adam ftw\n",
    "\n",
    "# Zero out gradients (typically ran during a training loop)\n",
    "optimizer.zero_grad() #helps prevent gradients from being counted twice!\n",
    "\n",
    "# Update Model Weights (after backprop)\n",
    "optimizer.step() #adjusts the network weights (function parameters) by the gradients computed in the Backward Pass (i.e. `loss.backward()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Learning Rate Scheduler -- review docs before implementing\n",
    "\n",
    "model = [Parameter(torch.randn(2, 2, requires_grad=True))]\n",
    "optimizer = SGD(model, 0.1)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Distribution of Output Predictions (Class Probs for a Forward Pass on Test Data) -- currently built for binary classes\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(x_test) #predict on test set\n",
    "y_pred = torch.sigmoid(y_pred) #if need class probabilities or some other transformation to raw preds (mapping to labels)\n",
    "\n",
    "y_pred = y_pred.detach().numpy().reshape(-1) #make a np array out of preds & reshape if necessary\n",
    "print(f\"Distribution of Probs: {list(np.histogram(y_pred, bins=4)[0])}\")\n",
    "print(f\"     Probability Bins: {list(np.histogram(y_pred, bins=4)[1])}\") #could tie this to some sort of visualization like a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Training & Evaluating Loops\n",
    "\n",
    "# Classification Specific (Discrete-ly Correct Preds + Accuracy as a Metric to eval on)\n",
    "def train(model, dataloader, n_epochs, loss_function, optimizer):\n",
    "    for epoch in range(n_epochs):\n",
    "        total = 0 \n",
    "        correct = 0 \n",
    "\n",
    "        # Expect Indicies + Data & Samples in this Version\n",
    "        for i, (samples, labels) in enumerate(dataloader):\n",
    "            # Clear Out Gradients per batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass + Compute Loss\n",
    "            pred = model(samples)\n",
    "            loss = loss_function(pred, labels)\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Num Correct Predictions -- Accuracy Metric \n",
    "            y_pred = torch.round(pred, decimals=6) #if binary output neuron (single), else can use argmax (softmax or multi-output)\n",
    "            y_pred = (torch.sigmoid(y_pred.reshape(-1).detach()) > 0.5).float() #logits into labels- round the output probs @ a 0.5 threshold\n",
    "            total += labels.size(0) #total num of instances seen (evaluated on all)\n",
    "            correct += (y_pred == labels).sum() #total num of correct instances (count restarted per epoch)\n",
    "\n",
    "        # Print Learning Info - per Epoch\n",
    "        accuracy = (100 * correct/total) #binary classification case\n",
    "        print(f'Epoch {epoch+1} - Training - Loss: {loss.item():.2f} & Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Below might be very wrong, was written in the abstract (for binary classification, why we have `threshold` param)\n",
    "def evaluate(model, loss_function, x_test, y_test, threshold):\n",
    "        model.eval() #no gradients here\n",
    "\n",
    "        # Get Preds + Compute Loss\n",
    "        y_pred = model(x_test)\n",
    "        y_pred = torch.sigmoid(y_pred) #squash values (binary classification scenario)\n",
    "        y_true = y_test.detach().numpy()\n",
    "        pred_performance = loss_function(y_pred.squeeze(), y_test)\n",
    "\n",
    "        # Compute Accuracy (wrt labels)\n",
    "        y_pred = (y_pred.reshape(-1).detach().numpy() > threshold).astype('float')\n",
    "        total = y_test.size(0)\n",
    "        correct += (y_pred == y_test)\n",
    "        accuracy = (correct/total) #on the order of 10s?\n",
    "\n",
    "        print(f'Test Set - Loss: {pred_performance.item() :.2f} & Accuracy: {accuracy :.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Model Parameters\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a Model `state_dict()`\n",
    "\n",
    "md = model.state_dict()\n",
    "md.keys() #list out the keys in the dictionary (names of model parameters, layer weights or biases, etc.)\n",
    "\n",
    "\n",
    "# Save Model Parameters to Dir -- Non-Useful? method of saving info about models\n",
    "meta = dict() #save shape info about saved parameters\n",
    "\n",
    "for k in md.keys():\n",
    "    # Specify Value per Parameter\n",
    "    v = md[k].numpy()\n",
    "    # print(k, \"-->\", v) #visualize the map from key to value\n",
    "\n",
    "    # Change Key Name for Better Saving\n",
    "    name = k.split(\".\")\n",
    "    name = \"-\".join(name)\n",
    "    np.savetxt(f\"model_parameters/{name}.csv\", v) #using numpy to nicely save\n",
    "    meta[name] = v.shape\n",
    "\n",
    "\n",
    "# Write out Metadata for each Params File\n",
    "with open('model_parameters/METADATA.csv', 'w') as f:\n",
    "    for key in meta.keys():\n",
    "        f.write(\"%s, %s\\n\" % (key, meta[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Models via `state_dict()`\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/SampleModel.pt\") #serialized version of model, need same architecture & device (cpu or gpu) for loading in later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in a `state_dict()` Saved Model \n",
    "\n",
    "model = FFN() #need specify the SAME architecture & device as saved model to load in the saved weights (including any potential finicky hyperparameters)\n",
    "model.load_state_dict(torch.load(\"../models/SampleModel.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in a PreTrained Torch Model \n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.vgg16(pretrained=True) #pre-trained models are available in each domain version of torch (will download the model so should save weights!)\n",
    "torch.save(model.state_dict(), 'models/vgg16.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Functions\n",
    "\n",
    "- Accessing GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Access Functions\n",
    "\n",
    "# Check if GPU is available \n",
    "torch.cuda.is_available() #true if not poor\n",
    "\n",
    "# Establish a Device for Sending Torch Objects to GPU or CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #alt method of doing above\n",
    "\n",
    "\n",
    "# Get Count of Available GPUs (if really rich)\n",
    "torch.cuda.device_count()\n",
    "\n",
    "# Get Name of GPU based on Index\n",
    "torch.cuda.get_device_name(0) #returns string name of GPU based on supplied index (i.e, 'NVIDIA GeForce RTX 3060 Ti')\n",
    "\n",
    "# Get Tuple of Device Compatibility Versions, Cryptic & Hopefully never use\n",
    "torch.cuda.get_device_capability(0) #reference this post for info- https://stackoverflow.com/questions/64535324/pytorch-get-device-capability-output-explanation\n",
    "\n",
    "# Move Tensor VERBOSELY to GPU -- assume tensor `t` is in memory\n",
    "t.cuda(device=0)\n",
    "# or\n",
    "t.cuda() #if no device to specify\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planting Seeds in Torch -- for Reproducibility & Testing Algorithms\n",
    "\n",
    "# All the Modules that Torch needs Set to make reproducible results\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "GLOBAL_SEED = 42 #what other default seed could there be?\n",
    "\n",
    "# This func needs to get run before any model calls (at instantiation & before any training, need include at each point!)\n",
    "def set_global_seed(seed=GLOBAL_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    torch.manual_seed(GLOBAL_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed(GLOBAL_SEED)\n",
    "    torch.cuda.manual_seed_all(GLOBAL_SEED) #not on GPU but still setting\n",
    "    torch.use_deterministic_algorithms(mode=True) #uses algorithms (where possible) that are deterministic\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def _init_fn(worker_id): #for dataloader's worker -- ensure fetching of data is deterministic if applicable (need to call by referencing as an arg in dataloader)\n",
    "    np.random.seed(int(GLOBAL_SEED))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f6b5390250318df4899fe5f9594fbcf229947067621a1fef8f9b9f256113137"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
